{"name":"node-crawler-scraper","version":"1.0.1","description":"Simple and powerful crawler. It scraps content and collects links from websites using request or phantomjs. The whole magic and simplicity is behind configuration.","main":"./lib/crawler.js","keywords":["scraper","scraping","phantomjs","cache","crawling","queue","nodejs","spider","jquery","crawler"],"scripts":{"test":"echo \"Error: no test specified\" && exit 1"},"dependencies":{"async":"^1.4.2","bluebird":"^2.10.2","cheerio":"^0.19.0","iconv-lite":"^0.4.13","lodash":"^3.10.1","make-url":"0.0.1","nock":"^2.15.0","node-horseman":"^2.6.0","request":"^2.64.0","underscore":"^1.8.3","url":"^0.11.0","urlencode":"^1.1.0"},"devDependencies":{"grunt":"^0.4.5","grunt-mocha-test":"^0.12.7","grunt-nodemon":"^0.4.0","mocha":"^2.3.3","should":"^4.6.5"},"license":"MIT","gitHead":"880245ac6d91ab4e28eeb6c8d8d3b23dd42234b2","versions":[{"number":"1.0.0","date":"2015-10-20T13:51:52.879Z"},{"number":"1.0.1","date":"2015-10-22T08:26:02.346Z"}],"readme":"ERROR: No README data found!","created":"2015-10-20T13:51:52.879Z","modified":"2015-10-22T08:26:02.346Z","lastPublisher":{"name":"cigolpl","email":"mrzepade@gmail.com"},"owners":[{"name":"cigolpl","email":"mrzepade@gmail.com"}],"other":{"_attachments":{},"_from":".","_id":"node-crawler-scraper","_nodeVersion":"4.1.2","_npmUser":{"name":"cigolpl","email":"mrzepade@gmail.com"},"_npmVersion":"2.14.4","_rev":"1-fa53cccc3a2da28f3e4985aaa9f36b0e","_shasum":"ef48ffb62270d221f4cd51f045b27e3d158996d4","author":{"name":"Mateusz Rzepa"},"directories":{"test":"tests"},"dist-tags":{"latest":"1.0.1"},"dist":{"shasum":"ef48ffb62270d221f4cd51f045b27e3d158996d4","tarball":"http://registry.npmjs.org/node-crawler-scraper/-/node-crawler-scraper-1.0.1.tgz"},"maintainers":[{"name":"cigolpl","email":"mrzepade@gmail.com"}],"readmeFilename":"","time":{"modified":"2015-10-22T08:26:02.346Z","created":"2015-10-20T13:51:52.879Z","1.0.0":"2015-10-20T13:51:52.879Z","1.0.1":"2015-10-22T08:26:02.346Z"}}}